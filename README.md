# DQN Variants for CartPole

All implementations use **Gymnasium** (the maintained fork of OpenAI Gym).

## Files Overview

### Main Implementations
1. **dqn_cartpole.py** - Standard DQN implementation
2. **double_dqn_cartpole.py** - Double DQN implementation  
3. **dueling_dqn_cartpole.py** - Dueling DQN implementation
4. **compare_all_dqn.py** - Compare all three algorithms

---

## Quick Start

### Option 1: Train Individual Algorithms
```bash
# Standard DQN
python dqn_cartpole.py

# Double DQN
python double_dqn_cartpole.py

# Dueling DQN
python dueling_dqn_cartpole.py
```

### Option 2: Compare All Three
```bash
python compare_all_dqn.py
```
This trains all three and creates comparison plots.

---

## Algorithm Comparison

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    STANDARD DQN                                 ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Input ‚Üí Hidden(128) ‚Üí Hidden(128) ‚Üí Output(2) ‚Üí Q-values      ‚îÇ
‚îÇ  [s‚ÇÄ, s‚ÇÅ, s‚ÇÇ, s‚ÇÉ]                           [Q(left), Q(right)]‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Update: Q_target = r + Œ≥ √ó max Q(s', a')                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚úì Simple, easy to understand                                  ‚îÇ
‚îÇ  ‚úó Can overestimate Q-values                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DOUBLE DQN                                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Q_eval:  Input ‚Üí Hidden ‚Üí Hidden ‚Üí Output ‚Üí Q-values          ‚îÇ
‚îÇ  Q_target: Input ‚Üí Hidden ‚Üí Hidden ‚Üí Output ‚Üí Q-values         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Update: Q_target = r + Œ≥ √ó Q_target(s', argmax Q_eval(s'))   ‚îÇ
‚îÇ           ‚Üë evaluate with target    ‚Üë select with eval         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚úì Reduces overestimation                                      ‚îÇ
‚îÇ  ‚úì More stable learning                                        ‚îÇ
‚îÇ  ‚úì Industry standard                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DUELING DQN                                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ         Input ‚Üí Hidden(128) ‚Üí Hidden(128)                       ‚îÇ
‚îÇ                                  ‚îú‚îÄ‚Üí V(s) [1 value]            ‚îÇ
‚îÇ                                  ‚îî‚îÄ‚Üí A(s,a) [2 values]         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Combine: Q(s,a) = V(s) + [A(s,a) - mean(A)]                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚úì Learns state value separately from action advantages        ‚îÇ
‚îÇ  ‚úì Faster convergence                                          ‚îÇ
‚îÇ  ‚úì Better generalization                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Simple Explanations

### Standard DQN
> "I'll learn Q-values for each action using one neural network"
- Like having one teacher grade all your work

### Double DQN  
> "I'll use one network to pick actions, another to evaluate them"
- Like having one teacher suggest answers, another grade them
- Prevents being overconfident

### Dueling DQN
> "I'll learn how good each STATE is, plus how much better each ACTION is"
- Splits learning into: "Is this a good situation?" + "Which action is best?"
- Learns faster because state quality is independent of actions

---

## Hyperparameters

```python
# Good defaults for CartPole
gamma = 0.99           # Discount factor
epsilon = 1.0          # Start with 100% exploration
epsilon_min = 0.01     # End with 1% exploration
epsilon_decay = 0.995  # Decay rate per episode
learning_rate = 0.001  # Network learning rate
batch_size = 64        # Experiences per update
memory_size = 100000   # Replay buffer size
hidden_layers = [128, 128]  # Network architecture
```



## üìù Requirements

```bash
pip install torch numpy gymnasium matplotlib
```

**Note:** We use `gymnasium` (not `gym`). 

For additional environments:
```bash
# For Box2D environments (LunarLander)
pip install gymnasium[box2d]

# For Atari games
pip install gymnasium[atari]
pip install gymnasium[accept-rom-license]
```

